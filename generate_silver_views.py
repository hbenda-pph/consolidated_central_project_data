# -*- coding: utf-8 -*-
"""
Generate Silver Views for All Tables

Este script genera autom√°ticamente las vistas Silver para todas las tablas
bas√°ndose en el an√°lisis de campos comunes y √∫nicos entre compa√±√≠as.

Paso 1: Crear vistas Silver por compa√±√≠a con layout normalizado
"""

from google.cloud import bigquery
import pandas as pd
import numpy as np
import time
from datetime import datetime
import warnings
from collections import defaultdict, Counter
import os
warnings.filterwarnings('ignore')

# print("‚úÖ Librer√≠as importadas correctamente")

# Importar configuraci√≥n centralizada
from config import *
from consolidation_status_manager import ConsolidationStatusManager
from consolidation_tracking_manager import ConsolidationTrackingManager

# print(f"üîß Configuraci√≥n:")
# print(f"   Proyecto: {PROJECT_SOURCE}")
# print(f"   Dataset: {DATASET_NAME}")
# print(f"   Tabla: {TABLE_NAME}")

# Crear cliente BigQuery con reconexi√≥n autom√°tica
def create_bigquery_client():
    """Crea cliente BigQuery con manejo de reconexi√≥n"""
    try:
        return bigquery.Client(project=PROJECT_SOURCE)
    except Exception as e:
        print(f"‚ö†Ô∏è  Error creando cliente BigQuery: {str(e)}")
        print("üîÑ Reintentando conexi√≥n...")
        time.sleep(5)  # Esperar 5 segundos
        return bigquery.Client(project=PROJECT_SOURCE)

try:
    client = create_bigquery_client()
    # print(f"‚úÖ Cliente BigQuery creado exitosamente para proyecto: {PROJECT_SOURCE}")
except Exception as e:
    print(f"‚ùå Error al crear cliente BigQuery: {str(e)}")
    raise

def get_companies_info():
    """
    Obtiene informaci√≥n de las compa√±√≠as desde la tabla companies
    """
    query = f"""
        SELECT
            company_id,
            company_name,
            company_new_name,
            company_project_id,
            company_bigquery_status,
            company_consolidated_status
        FROM `{PROJECT_SOURCE}.{DATASET_NAME}.{TABLE_NAME}`
        WHERE company_fivetran_status = TRUE
          AND company_bigquery_status = TRUE
        ORDER BY company_id
    """

    try:
        query_job = client.query(query)
        results = query_job.result()
        companies_df = pd.DataFrame([dict(row) for row in results])
        # print(f"‚úÖ Informaci√≥n de compa√±√≠as obtenida: {len(companies_df)} registros")
        return companies_df
    except Exception as e:
        print(f"‚ùå Error al obtener informaci√≥n de compa√±√≠as: {str(e)}")
        raise

def get_table_fields_with_types(project_id, table_name):
    """
    Obtiene informaci√≥n de campos con tipos de datos de una tabla espec√≠fica
    """
    dataset_name = f"servicetitan_{project_id.replace('-', '_')}"
    try:
        query = f"""
            SELECT
                table_catalog,
                table_schema,
                table_name,
                column_name,
                data_type,
                is_nullable,
                column_default,
                ordinal_position
            FROM `{project_id}.{dataset_name}.INFORMATION_SCHEMA.COLUMNS`
            WHERE table_name = '{table_name}'
            ORDER BY ordinal_position
        """

        query_job = client.query(query)
        results = query_job.result()
        fields_df = pd.DataFrame([dict(row) for row in results])
        return fields_df
    except Exception as e:
        print(f"‚ö†Ô∏è  Error al obtener campos de {project_id}.{dataset_name}.{table_name}: {str(e)}")
        return pd.DataFrame()

def analyze_table_fields_across_companies(table_name):
    """
    Analiza los campos de una tabla espec√≠fica en todas las compa√±√≠as
    """
    print(f"\nüîç ANALIZANDO TABLA: {table_name}")
    print("=" * 80)
    
    companies_df = get_companies_info()
    table_analysis_results = []
    all_table_fields = set()
    
    for idx, row in companies_df.iterrows():
        company_id = row['company_id']
        company_name = row['company_name']
        project_id = row['company_project_id']
        
        if project_id is None:
            continue
        
        # Obtener campos de la tabla
        fields_df = get_table_fields_with_types(project_id, table_name)
        
        if fields_df.empty:
            print(f"  ‚ö†Ô∏è  {company_name}: Tabla '{table_name}' no encontrada")
            continue
            
        # Filtrar campos _fivetran (campos del ETL que deben quedarse solo en Bronze)
        filtered_fields_df = fields_df[~fields_df['column_name'].str.startswith('_fivetran')]
        fields_list = filtered_fields_df['column_name'].tolist()
        field_count = len(fields_list)
        
        # print(f"  ‚úÖ {company_name}: {field_count} campos (filtrados _fivetran)")
        
        # Guardar informaci√≥n
        table_analysis_results.append({
            'company_id': company_id,
            'company_name': company_name,
            'project_id': project_id,
            'field_count': field_count,
            'fields': fields_list,
            'fields_df': filtered_fields_df
        })
        
        all_table_fields.update(fields_list)
    
    if not table_analysis_results:
        print(f"  ‚ùå No se encontraron datos para la tabla '{table_name}'")
        return None
    
    # Analizar campos comunes y √∫nicos
    field_frequency = Counter()
    for result in table_analysis_results:
        field_frequency.update(result['fields'])
    
    # Determinar campos comunes (presentes en todas las compa√±√≠as)
    total_companies = len(table_analysis_results)
    common_fields = []
    partial_fields = []
    
    for field, count in field_frequency.items():
        if count == total_companies:
            common_fields.append(field)
        else:
            partial_fields.append(field)
    
    # Analizar tipos de datos
    print(f"\nüîç ANALIZANDO TIPOS DE DATOS...")
    field_consensus, type_conflicts = analyze_data_types_for_table(table_analysis_results)
    
    print(f"\nüìä AN√ÅLISIS DE CAMPOS PARA '{table_name}':")
    print(f"  Total de compa√±√≠as con la tabla: {total_companies}")
    print(f"  Total de campos √∫nicos: {len(all_table_fields)}")
    print(f"  Campos comunes: {len(common_fields)}")
    print(f"  Campos parciales: {len(partial_fields)}")
    print(f"  Campos sin conflicto de tipo: {len(field_consensus)}")
    print(f"  Campos con conflicto de tipo: {len(type_conflicts)}")
    
    # if common_fields:
    #     print(f"\n‚úÖ CAMPOS COMUNES:")
    #     for field in common_fields:
    #         print(f"    - {field}")
    
    if partial_fields:
        print(f"\n‚ö†Ô∏è  CAMPOS PARCIALES:")
        for field in partial_fields:
            count = field_frequency[field]
            print(f"    - {field}: {count}/{total_companies} compa√±√≠as")
    
    if type_conflicts:
        print(f"\n‚ö†Ô∏è  CONFLICTOS DE TIPO:")
        for field_name, conflict in type_conflicts.items():
            print(f"    - {field_name}: {', '.join(conflict['types'])} ‚Üí {conflict['consensus_type']}")
    
    return {
        'table_name': table_name,
        'total_companies': total_companies,
        'all_fields': sorted(all_table_fields),
        'common_fields': sorted(common_fields),
        'partial_fields': sorted(partial_fields),
        'field_consensus': field_consensus,
        'type_conflicts': type_conflicts,
        'company_results': table_analysis_results,
        'field_frequency': field_frequency
    }

def generate_silver_view_sql(table_analysis, company_result):
    """
    Genera el SQL para crear una vista Silver para una compa√±√≠a espec√≠fica
    Incluye normalizaci√≥n de tipos de datos
    """
    table_name = table_analysis['table_name']
    company_name = company_result['company_name']
    project_id = company_result['project_id']
    
    # Obtener campos de esta compa√±√≠a con sus tipos
    company_fields_df = company_result['fields_df']
    company_fields = {row['column_name']: row['data_type'] for _, row in company_fields_df.iterrows()}
    company_field_names = set(company_fields.keys())
    
    silver_fields = []
    processed_fields = set()  # Para evitar duplicados
    
    # 1. Procesar campos comunes (sin conflictos de tipo)
    for field_name, field_info in table_analysis['field_consensus'].items():
        # SOLO incluir campos que existen en esta compa√±√≠a y no se han procesado
        if field_name not in company_field_names or field_name in processed_fields:
            continue
            
        target_type = field_info['type']
        source_type = company_fields.get(field_name)
        
        # Si el campo no existe en esta compa√±√≠a, saltarlo
        if source_type is None:
            continue
        
        # SIEMPRE aplicar cast para asegurar consistencia de tipos
        cast_expression = generate_cast_for_field(field_name, source_type, target_type)
        silver_fields.append(f"    {cast_expression} as {field_name}")
        processed_fields.add(field_name)
        
    # 2. Procesar campos con conflictos de tipo (solo los no procesados)
    for field_name, conflict_info in table_analysis['type_conflicts'].items():
        # SOLO incluir campos que existen en esta compa√±√≠a y no se han procesado
        if field_name not in company_field_names or field_name in processed_fields:
            continue
            
        target_type = conflict_info['consensus_type']
        source_type = company_fields.get(field_name)
        
        # Si el campo no existe en esta compa√±√≠a, saltarlo
        if source_type is None:
            continue
        
        # SIEMPRE aplicar cast para asegurar consistencia de tipos
        cast_expression = generate_cast_for_field(field_name, source_type, target_type)
        silver_fields.append(f"    {cast_expression} as {field_name}")
        processed_fields.add(field_name)
    
    # 3. Procesar campos faltantes (con valores por defecto)
    # IMPORTANTE: Mantener layout consistente para UNION ALL
    all_analyzed_fields = set(table_analysis['field_consensus'].keys()) | set(table_analysis['type_conflicts'].keys())
    missing_fields = all_analyzed_fields - company_field_names
    
    for field_name in missing_fields:
        # Determinar tipo objetivo
        if field_name in table_analysis['field_consensus']:
            target_type = table_analysis['field_consensus'][field_name]['type']
        else:
            target_type = table_analysis['type_conflicts'][field_name]['consensus_type']
        
        default_value = get_default_value_for_type(target_type)
        silver_fields.append(f"    {default_value} as {field_name}")
    
    # 4. Metadata fields (eliminados - no necesarios en vistas por compa√±√≠a)
    
    # Crear SQL
    dataset_name = f"servicetitan_{project_id.replace('-', '_')}"
    view_name = f"vw_{table_name}"
    
    # Validar que hay campos para procesar
    if not silver_fields:
        print(f"  ‚ö†Ô∏è  No hay campos para procesar en {company_name} - {table_name}")
        return None
    
    # Agregar comas entre campos (excepto el √∫ltimo)
    fields_with_commas = []
    for i, field in enumerate(silver_fields):
        if i < len(silver_fields) - 1:
            fields_with_commas.append(field + ",")
        else:
            fields_with_commas.append(field)
    
    # Crear el contenido de campos con saltos de l√≠nea
    fields_content = '\n'.join(fields_with_commas)
    
    sql = f"""-- Vista Silver para {company_name} - Tabla {table_name}
-- Generada autom√°ticamente el {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
-- Incluye normalizaci√≥n de tipos de datos

CREATE OR REPLACE VIEW `{project_id}.silver.{view_name}` AS (
SELECT
{fields_content}
FROM `{project_id}.{dataset_name}.{table_name}`
);
"""
    
    return sql

# Funciones para an√°lisis de tipos de datos
def analyze_data_types_for_table(table_analysis_results):
    """
    Analiza las diferencias de tipos de datos para una tabla
    """
    field_type_analysis = defaultdict(list)
    
    # Recopilar informaci√≥n de tipos por campo
    for result in table_analysis_results:
        company_name = result['company_name']
        project_id = result['project_id']
        fields_df = result['fields_df']
        
        for _, field in fields_df.iterrows():
            field_name = field['column_name']
            data_type = field['data_type']
            
            field_type_analysis[field_name].append({
                'company_name': company_name,
                'project_id': project_id,
                'data_type': data_type,
                'is_nullable': field['is_nullable']
            })
    
    # Analizar diferencias de tipos
    type_conflicts = {}
    field_consensus = {}
    
    for field_name, type_info_list in field_type_analysis.items():
        unique_types = list(set([info['data_type'] for info in type_info_list]))
        
        if len(unique_types) > 1:
            # Hay conflicto de tipos
            type_conflicts[field_name] = {
                'types': unique_types,
                'companies': type_info_list,
                'consensus_type': determine_consensus_type(unique_types, type_info_list)
            }
    else:
            # No hay conflicto
            field_consensus[field_name] = {
                'type': unique_types[0],
                'companies': type_info_list
            }
    
    return field_consensus, type_conflicts

def determine_consensus_type(types, type_info_list):
    """
    Determina el tipo de consenso para un campo con conflictos
    """
    type_priority = {
        'STRING': 1,      # M√°s flexible
        'INT64': 2,       
        'FLOAT64': 3,     
        'BOOL': 4,        
        'DATE': 5,        
        'DATETIME': 6,    
        'TIMESTAMP': 7,   
        'JSON': 8,        
        'BYTES': 9        
    }
    
    # Si hay STRING, usar STRING (m√°s flexible)
    if 'STRING' in types:
        return 'STRING'
    
    # Si hay FLOAT64, usar FLOAT64 (puede contener enteros)
    if 'FLOAT64' in types:
        return 'FLOAT64'
    
    # Si hay INT64, usar INT64
    if 'INT64' in types:
        return 'INT64'
    
    # Para otros tipos, usar el de mayor prioridad
    min_priority = min([type_priority.get(t, 999) for t in types])
    for t in types:
        if type_priority.get(t, 999) == min_priority:
            return t
    
    return types[0]

def generate_cast_for_field(field_name, source_type, target_type):
    """
    Genera la expresi√≥n CAST apropiada para un campo
    """
    if source_type == target_type:
        return field_name
    
    # Mapeo de conversiones seguras
    safe_casts = {
        ('INT64', 'STRING'): f"CAST({field_name} AS STRING)",
        ('INT64', 'FLOAT64'): f"CAST({field_name} AS FLOAT64)",
        ('FLOAT64', 'STRING'): f"CAST({field_name} AS STRING)",
        ('STRING', 'INT64'): f"SAFE_CAST({field_name} AS INT64)",
        ('STRING', 'FLOAT64'): f"SAFE_CAST({field_name} AS FLOAT64)",
        ('STRING', 'BOOL'): f"SAFE_CAST({field_name} AS BOOL)",
        ('BOOL', 'STRING'): f"CAST({field_name} AS STRING)",
        ('DATE', 'STRING'): f"CAST({field_name} AS STRING)",
        ('DATETIME', 'STRING'): f"CAST({field_name} AS STRING)",
        ('TIMESTAMP', 'STRING'): f"CAST({field_name} AS STRING)",
        # JSON a otros tipos - usar TO_JSON_STRING para convertir a STRING
        ('JSON', 'STRING'): f"COALESCE(TO_JSON_STRING({field_name}), '')",
        ('JSON', 'INT64'): f"COALESCE(SAFE_CAST(TO_JSON_STRING({field_name}) AS INT64), 0)",
        ('JSON', 'FLOAT64'): f"COALESCE(SAFE_CAST(TO_JSON_STRING({field_name}) AS FLOAT64), 0.0)"
    }
    
    cast_key = (source_type, target_type)
    if cast_key in safe_casts:
        return safe_casts[cast_key]
    
    # Para conversiones no seguras, usar SAFE_CAST con valor por defecto
    return f"COALESCE(SAFE_CAST({field_name} AS {target_type}), {get_default_value_for_type(target_type)})"

def get_default_value_for_type(data_type):
    """Obtiene el valor por defecto para un tipo de datos"""
    defaults = {
        'STRING': "''",
        'INT64': '0',
        'FLOAT64': '0.0',
        'BOOL': 'FALSE',
        'DATE': 'NULL',
        'DATETIME': 'NULL',
        'TIMESTAMP': 'NULL',
        'JSON': 'NULL',
        'BYTES': 'NULL'
    }
    return defaults.get(data_type, 'NULL')

def generate_all_silver_views(force_recreate=False):
    """
    Genera vistas Silver para todas las tablas identificadas con seguimiento de estados
    
    Args:
        force_recreate (bool): Si True, recrea todas las vistas sin importar el estado de consolidaci√≥n
    """
    print("üöÄ Iniciando generaci√≥n de vistas Silver para todas las tablas")
    
    # Inicializar gestores
    status_manager = ConsolidationStatusManager()
    tracking_manager = ConsolidationTrackingManager()
    
    # Obtener compa√±√≠as pendientes de consolidaci√≥n
    pending_companies = status_manager.get_companies_for_consolidation()
    
    if pending_companies.empty:
        print("‚ÑπÔ∏è  No hay compa√±√≠as pendientes de consolidaci√≥n")
        return {}, {}
    
    print(f"üìã Compa√±√≠as a procesar: {len(pending_companies)}")
    
    # Usar configuraci√≥n centralizada
    all_tables = TABLES_TO_PROCESS
    
    if force_recreate:
        print("üîÑ MODO FORZADO: Recreando todas las vistas sin importar estado de consolidaci√≥n")
        tables_to_process = all_tables
    else:
        print("üìã MODO NORMAL: Filtrando tablas ya consolidadas")
        tables_to_process = tracking_manager.get_tables_to_process(all_tables)
    
    all_results = {}
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Crear directorio para archivos SQL
    output_dir = f"{OUTPUT_BASE_DIR}/silver_views_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)
    
    if not tables_to_process:
        print("‚úÖ Todas las tablas est√°n 100% consolidadas. No hay nada que procesar.")
        return {}, {}
    
    print(f"üöÄ INICIANDO GENERACI√ìN DE VISTAS SILVER")
    print(f"üìÅ Directorio de salida: {output_dir}")
    print(f"üìã Tablas a procesar: {len(tables_to_process)}")
    print("=" * 80)
    
    for table_name in tables_to_process:
        print(f"\nüîÑ Procesando tabla: {table_name}")
        
        # Verificar si la tabla ya est√° 100% consolidada (solo en modo normal)
        if not force_recreate:
            completion_status = tracking_manager.get_table_completion_status(table_name)
            
            if completion_status['is_fully_consolidated']:
                print(f"  ‚è≠Ô∏è  Saltando tabla '{table_name}' - 100% consolidada ({completion_status['completion_rate']:.1f}%)")
                continue
        else:
            # En modo forzado, mostrar estado pero no saltar
            completion_status = tracking_manager.get_table_completion_status(table_name)
        
        print(f"  üìä Estado actual: {completion_status['completion_rate']:.1f}% completada")
        print(f"     ‚úÖ √âxitos: {completion_status['success_count']}")
        print(f"     ‚ùå Errores: {completion_status['error_count']}")
        print(f"     ‚ö†Ô∏è  No existe: {completion_status['missing_count']}")
        
        # Analizar campos de la tabla
        table_analysis = analyze_table_fields_across_companies(table_name)
        
        if table_analysis is None:
            print(f"  ‚è≠Ô∏è  Saltando tabla '{table_name}' - no se encontraron datos")
            # Registrar estado 0 para todas las compa√±√≠as (tabla no existe)
            for _, company in pending_companies.iterrows():
                tracking_manager.update_status(
                    company_id=company['company_id'],
                    table_name=table_name,
                    status=0,
                    notes="Tabla no existe en esta compa√±√≠a"
                )
            continue
        
        all_results[table_name] = table_analysis
        
        # Obtener compa√±√≠as que tienen la tabla
        companies_with_table = {result['company_name'] for result in table_analysis['company_results']}
        
        # Registrar estado 0 para compa√±√≠as que no tienen la tabla
        for _, company in pending_companies.iterrows():
            company_name = company['company_name']
            if company_name not in companies_with_table:
                tracking_manager.update_status(
                    company_id=company['company_id'],
                    table_name=table_name,
                    status=0,
                    notes="Tabla no existe en esta compa√±√≠a"
                )
        
        # Generar vistas Silver para cada compa√±√≠a
        company_sql_files = []
        
        for company_result in table_analysis['company_results']:
            company_name = company_result['company_name']
            project_id = company_result['project_id']
            
            # Generar y ejecutar SQL directamente
            sql_content = generate_silver_view_sql(table_analysis, company_result)
            
            # Validar que se gener√≥ SQL v√°lido
            if sql_content is None:
                print(f"    ‚ö†Ô∏è  No se pudo generar SQL para {company_name}")
                tracking_manager.update_status(
                    company_id=company_result['company_id'],
                    table_name=table_name,
                    status=2,
                    notes="Error generando SQL - sin campos v√°lidos"
                )
                continue
            
            # Ejecutar vista directamente en BigQuery con reconexi√≥n autom√°tica
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    if attempt > 0:
                        print(f"    üîÑ Reintento {attempt + 1}/{max_retries} para {company_name}")
                        # Recrear cliente en caso de error de autenticaci√≥n
                        global client
                        client = create_bigquery_client()
                        time.sleep(2)  # Esperar antes del reintento
                    
                    print(f"    üîÑ Creando vista: {project_id}.silver.vw_{table_name}")
                    query_job = client.query(sql_content)
                    query_job.result()  # Esperar a que termine
                    print(f"    ‚úÖ Vista creada: {company_name}")
                    company_sql_files.append(f"SUCCESS: {company_name}")
                    
                    # Actualizar tracking
                    tracking_manager.update_status(
                        company_id=company_result['company_id'],
                        table_name=table_name,
                        status=1,
                        notes=f"Vista creada exitosamente en {project_id}.silver"
                    )
                    break  # √âxito, salir del loop de reintentos
                    
                except Exception as e:
                    error_msg = str(e)
                    if attempt == max_retries - 1:  # √öltimo intento
                        print(f"    ‚ùå Error final creando vista {company_name}: {error_msg}")
                        company_sql_files.append(f"ERROR: {company_name}")
                        
                        # Actualizar tracking con error
                        tracking_manager.update_status(
                            company_id=company_result['company_id'],
                            table_name=table_name,
                            status=2,
                            error_message=error_msg,
                            notes=f"Error al crear vista en {project_id}.silver"
                        )
                    else:
                        print(f"    ‚ö†Ô∏è  Error en intento {attempt + 1}: {error_msg}")
                        continue
        
        # Crear archivo consolidado para la tabla
        consolidated_filename = f"{output_dir}/consolidated_{table_name}_analysis.sql"
        with open(consolidated_filename, 'w', encoding='utf-8') as f:
            f.write(f"-- An√°lisis consolidado para tabla: {table_name}\n")
            f.write(f"-- Generado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write(f"-- Resumen:\n")
            f.write(f"-- Total compa√±√≠as: {table_analysis['total_companies']}\n")
            f.write(f"-- Campos comunes: {len(table_analysis['common_fields'])}\n")
            f.write(f"-- Campos parciales: {len(table_analysis['partial_fields'])}\n\n")
            
            f.write(f"-- Campos comunes:\n")
            for field in table_analysis['common_fields']:
                f.write(f"--   - {field}\n")
            
            f.write(f"\n-- Campos parciales:\n")
            for field in table_analysis['partial_fields']:
                count = table_analysis['field_frequency'][field]
                f.write(f"--   - {field}: {count}/{table_analysis['total_companies']} compa√±√≠as\n")
            
            f.write(f"\n-- Archivos SQL generados:\n")
            for sql_file in company_sql_files:
                f.write(f"--   - {sql_file}\n")
        
        print(f"    üìÑ Archivo consolidado: {consolidated_filename}")
    
    # Generar resumen final
    summary_filename = f"{output_dir}/GENERATION_SUMMARY.md"
    with open(summary_filename, 'w', encoding='utf-8') as f:
        f.write(f"# Resumen de Generaci√≥n de Vistas Silver\n\n")
        f.write(f"**Fecha de generaci√≥n:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write(f"## Tablas Procesadas\n\n")
        f.write(f"Total de tablas procesadas: {len(all_results)}\n\n")
        
        for table_name, analysis in all_results.items():
            f.write(f"### {table_name}\n")
            f.write(f"- Compa√±√≠as con la tabla: {analysis['total_companies']}\n")
            f.write(f"- Campos comunes: {len(analysis['common_fields'])}\n")
            f.write(f"- Campos parciales: {len(analysis['partial_fields'])}\n\n")
        
        f.write(f"## Archivos Generados\n\n")
        f.write(f"Para cada tabla se generaron:\n")
        f.write(f"- Un archivo SQL por compa√±√≠a: `silver_{{table_name}}_{{project_id}}.sql`\n")
        f.write(f"- Un archivo de an√°lisis consolidado: `consolidated_{{table_name}}_analysis.sql`\n\n")
        
        f.write(f"## Pr√≥ximos Pasos\n\n")
        f.write(f"1. Revisar los archivos SQL generados\n")
        f.write(f"2. Ejecutar las vistas Silver en cada proyecto de compa√±√≠a\n")
        f.write(f"3. Crear las vistas consolidadas en el proyecto central\n")
    
    # Actualizar estados de compa√±√≠as procesadas
    print(f"\nüìä Actualizando estados de consolidaci√≥n...")
    for _, company in pending_companies.iterrows():
        company_id = company['company_id']
        company_name = company['company_name']
        
        # Verificar si la compa√±√≠a fue procesada exitosamente
        company_success = True
        for table_name in tables_to_process:
            if table_name in all_results:
                # Verificar si esta compa√±√≠a tiene datos para esta tabla
                company_has_table = any(
                    result['company_id'] == company_id 
                    for result in all_results[table_name]['company_results']
                )
                if not company_has_table:
                    # No es un error si la compa√±√≠a no tiene cierta tabla
                    continue
        
        # Actualizar estado
        if company_success:
            status_manager.update_company_status(company_id, status_manager.STATUS['COMPLETED'])
            # print(f"  ‚úÖ {company_name}: Estado actualizado a COMPLETED")
        else:
            status_manager.update_company_status(company_id, status_manager.STATUS['ERROR'])
            print(f"  ‚ùå {company_name}: Estado actualizado a ERROR")
    
    # Mostrar resumen de estados
    status_manager.print_consolidation_summary()
    
    # Mostrar reporte de consolidaci√≥n
    tracking_manager.print_consolidation_report()
    
    # Mostrar resumen de tablas saltadas
    print(f"\nüìã RESUMEN DE TABLAS:")
    print("=" * 50)
    
    processed_count = 0
    skipped_count = 0
    
    for table_name in all_tables:
        completion_status = tracking_manager.get_table_completion_status(table_name)
        
        if not force_recreate and completion_status['is_fully_consolidated']:
            skipped_count += 1
            print(f"  ‚è≠Ô∏è  {table_name}: SALTADA - 100% consolidada")
        else:
            processed_count += 1
            if force_recreate:
                print(f"  üîÑ {table_name}: RECREADA - {completion_status['completion_rate']:.1f}% completada")
            else:
                print(f"  üîÑ {table_name}: PROCESADA - {completion_status['completion_rate']:.1f}% completada")
    
    print(f"\nüéØ GENERACI√ìN COMPLETADA")
    print(f"üìÅ Directorio: {output_dir}")
    print(f"üìä Tablas procesadas: {processed_count}")
    print(f"‚è≠Ô∏è  Tablas saltadas: {skipped_count}")
    print(f"üìÑ Resumen: {summary_filename}")
    print(f"üìä Tracking: Tabla companies_consolidated actualizada")
    
    return all_results, output_dir

if __name__ == "__main__":
    import sys
    
    # Verificar si se solicita recreaci√≥n forzada
    force_recreate = len(sys.argv) > 1 and sys.argv[1].lower() in ['--force', '-f', 'force']
    
    if force_recreate:
        print("üîÑ MODO FORZADO ACTIVADO: Recreando todas las vistas Silver")
        print("‚ö†Ô∏è  ADVERTENCIA: Esto puede tomar mucho tiempo")
        confirm = input("¬øContinuar? (y/N): ").strip().lower()
        if confirm != 'y':
            print("‚ùå Operaci√≥n cancelada")
            sys.exit(0)
    
    # Ejecutar generaci√≥n
    results, output_dir = generate_all_silver_views(force_recreate=force_recreate)
    
    print(f"\n‚úÖ Script completado exitosamente!")
    print(f"üìÅ Revisa los archivos en: {output_dir}")
