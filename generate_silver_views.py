# -*- coding: utf-8 -*-
"""
Generate Silver Views for All Tables

Este script genera autom√°ticamente las vistas Silver para todas las tablas
bas√°ndose en el an√°lisis de campos comunes y √∫nicos entre compa√±√≠as.

Paso 1: Crear vistas Silver por compa√±√≠a con layout normalizado
"""

from google.cloud import bigquery
import pandas as pd
import numpy as np
from datetime import datetime
import warnings
from collections import defaultdict, Counter
import os
warnings.filterwarnings('ignore')

# print("‚úÖ Librer√≠as importadas correctamente")

# Importar configuraci√≥n centralizada
from config import *
from consolidation_status_manager import ConsolidationStatusManager

# print(f"üîß Configuraci√≥n:")
# print(f"   Proyecto: {PROJECT_SOURCE}")
# print(f"   Dataset: {DATASET_NAME}")
# print(f"   Tabla: {TABLE_NAME}")

# Crear cliente de BigQuery
try:
    client = bigquery.Client(project=PROJECT_SOURCE)
    # print(f"‚úÖ Cliente BigQuery creado exitosamente para proyecto: {PROJECT_SOURCE}")
except Exception as e:
    print(f"‚ùå Error al crear cliente BigQuery: {str(e)}")
    raise

def get_companies_info():
    """
    Obtiene informaci√≥n de las compa√±√≠as desde la tabla companies
    """
    query = f"""
        SELECT
            company_id,
            company_name,
            company_new_name,
            company_project_id,
            company_bigquery_status,
            company_consolidated_status
        FROM `{PROJECT_SOURCE}.{DATASET_NAME}.{TABLE_NAME}`
        WHERE company_bigquery_status IS NOT NULL
        ORDER BY company_id
    """

    try:
        query_job = client.query(query)
        results = query_job.result()
        companies_df = pd.DataFrame([dict(row) for row in results])
        # print(f"‚úÖ Informaci√≥n de compa√±√≠as obtenida: {len(companies_df)} registros")
        return companies_df
    except Exception as e:
        print(f"‚ùå Error al obtener informaci√≥n de compa√±√≠as: {str(e)}")
        raise

def get_table_fields_with_types(project_id, table_name):
    """
    Obtiene informaci√≥n de campos con tipos de datos de una tabla espec√≠fica
    """
    dataset_name = f"servicetitan_{project_id.replace('-', '_')}"
    try:
        query = f"""
            SELECT
                table_catalog,
                table_schema,
                table_name,
                column_name,
                data_type,
                is_nullable,
                column_default,
                ordinal_position
            FROM `{project_id}.{dataset_name}.INFORMATION_SCHEMA.COLUMNS`
            WHERE table_name = '{table_name}'
            ORDER BY ordinal_position
        """

        query_job = client.query(query)
        results = query_job.result()
        fields_df = pd.DataFrame([dict(row) for row in results])
        return fields_df
    except Exception as e:
        print(f"‚ö†Ô∏è  Error al obtener campos de {project_id}.{dataset_name}.{table_name}: {str(e)}")
        return pd.DataFrame()

def analyze_table_fields_across_companies(table_name):
    """
    Analiza los campos de una tabla espec√≠fica en todas las compa√±√≠as
    """
    print(f"\nüîç ANALIZANDO TABLA: {table_name}")
    print("=" * 80)
    
    companies_df = get_companies_info()
    table_analysis_results = []
    all_table_fields = set()
    
    for idx, row in companies_df.iterrows():
        company_id = row['company_id']
        company_name = row['company_name']
        project_id = row['company_project_id']
        
        if project_id is None:
            continue
            
        # Obtener campos de la tabla
        fields_df = get_table_fields_with_types(project_id, table_name)
        
        if fields_df.empty:
            print(f"  ‚ö†Ô∏è  {company_name}: Tabla '{table_name}' no encontrada")
            continue
            
        # Filtrar campos _fivetran (campos del ETL que deben quedarse solo en Bronze)
        filtered_fields_df = fields_df[~fields_df['column_name'].str.startswith('_fivetran')]
        fields_list = filtered_fields_df['column_name'].tolist()
        field_count = len(fields_list)
        
        # print(f"  ‚úÖ {company_name}: {field_count} campos (filtrados _fivetran)")
        
        # Guardar informaci√≥n
        table_analysis_results.append({
            'company_id': company_id,
            'company_name': company_name,
            'project_id': project_id,
            'field_count': field_count,
            'fields': fields_list,
            'fields_df': filtered_fields_df
        })
        
        all_table_fields.update(fields_list)
    
    if not table_analysis_results:
        print(f"  ‚ùå No se encontraron datos para la tabla '{table_name}'")
        return None
    
    # Analizar campos comunes y √∫nicos
    field_frequency = Counter()
    for result in table_analysis_results:
        field_frequency.update(result['fields'])
    
    # Determinar campos comunes (presentes en todas las compa√±√≠as)
    total_companies = len(table_analysis_results)
    common_fields = []
    partial_fields = []
    
    for field, count in field_frequency.items():
        if count == total_companies:
            common_fields.append(field)
        else:
            partial_fields.append(field)
    
    # Analizar tipos de datos
    print(f"\nüîç ANALIZANDO TIPOS DE DATOS...")
    field_consensus, type_conflicts = analyze_data_types_for_table(table_analysis_results)
    
    print(f"\nüìä AN√ÅLISIS DE CAMPOS PARA '{table_name}':")
    print(f"  Total de compa√±√≠as con la tabla: {total_companies}")
    print(f"  Total de campos √∫nicos: {len(all_table_fields)}")
    print(f"  Campos comunes: {len(common_fields)}")
    print(f"  Campos parciales: {len(partial_fields)}")
    print(f"  Campos sin conflicto de tipo: {len(field_consensus)}")
    print(f"  Campos con conflicto de tipo: {len(type_conflicts)}")
    
    # if common_fields:
    #     print(f"\n‚úÖ CAMPOS COMUNES:")
    #     for field in common_fields:
    #         print(f"    - {field}")
    
    if partial_fields:
        print(f"\n‚ö†Ô∏è  CAMPOS PARCIALES:")
        for field in partial_fields:
            count = field_frequency[field]
            print(f"    - {field}: {count}/{total_companies} compa√±√≠as")
    
    if type_conflicts:
        print(f"\n‚ö†Ô∏è  CONFLICTOS DE TIPO:")
        for field_name, conflict in type_conflicts.items():
            print(f"    - {field_name}: {', '.join(conflict['types'])} ‚Üí {conflict['consensus_type']}")
    
    return {
        'table_name': table_name,
        'total_companies': total_companies,
        'all_fields': sorted(all_table_fields),
        'common_fields': sorted(common_fields),
        'partial_fields': sorted(partial_fields),
        'field_consensus': field_consensus,
        'type_conflicts': type_conflicts,
        'company_results': table_analysis_results,
        'field_frequency': field_frequency
    }

def generate_silver_view_sql(table_analysis, company_result):
    """
    Genera el SQL para crear una vista Silver para una compa√±√≠a espec√≠fica
    Incluye normalizaci√≥n de tipos de datos
    """
    table_name = table_analysis['table_name']
    company_name = company_result['company_name']
    project_id = company_result['project_id']
    
    # Obtener campos de esta compa√±√≠a con sus tipos
    company_fields_df = company_result['fields_df']
    company_fields = {row['column_name']: row['data_type'] for _, row in company_fields_df.iterrows()}
    company_field_names = set(company_fields.keys())
    
    silver_fields = []
    
    # 1. Procesar campos comunes (sin conflictos de tipo)
    for field_name, field_info in table_analysis['field_consensus'].items():
        # SOLO incluir campos que existen en esta compa√±√≠a
        if field_name not in company_field_names:
            continue
            
        target_type = field_info['type']
        source_type = company_fields.get(field_name)
        
        # Si el campo no existe en esta compa√±√≠a, saltarlo
        if source_type is None:
            continue
        
        if source_type == target_type:
            silver_fields.append(f"    {field_name}")
        else:
            cast_expression = generate_cast_for_field(field_name, source_type, target_type)
            silver_fields.append(f"    {cast_expression} as {field_name}")
        
    # 2. Procesar campos con conflictos de tipo
    for field_name, conflict_info in table_analysis['type_conflicts'].items():
        # SOLO incluir campos que existen en esta compa√±√≠a
        if field_name not in company_field_names:
            continue
            
        target_type = conflict_info['consensus_type']
        source_type = company_fields.get(field_name)
        
        # Si el campo no existe en esta compa√±√≠a, saltarlo
        if source_type is None:
            continue
        
        if source_type == target_type:
            silver_fields.append(f"    {field_name}")
        else:
            cast_expression = generate_cast_for_field(field_name, source_type, target_type)
            silver_fields.append(f"    {cast_expression} as {field_name}")
    
    # 3. Procesar campos faltantes (con valores por defecto)
    # IMPORTANTE: Mantener layout consistente para UNION ALL
    all_analyzed_fields = set(table_analysis['field_consensus'].keys()) | set(table_analysis['type_conflicts'].keys())
    missing_fields = all_analyzed_fields - company_field_names
    
    for field_name in missing_fields:
        # Determinar tipo objetivo
        if field_name in table_analysis['field_consensus']:
            target_type = table_analysis['field_consensus'][field_name]['type']
        else:
            target_type = table_analysis['type_conflicts'][field_name]['consensus_type']
        
        default_value = get_default_value_for_type(target_type)
        silver_fields.append(f"    {default_value} as {field_name}")
    
    # 4. Metadata fields (eliminados - no necesarios en vistas por compa√±√≠a)
    
    # Crear SQL
    dataset_name = f"servicetitan_{project_id.replace('-', '_')}"
    view_name = f"vw_{table_name}"
    
    # Agregar comas entre campos (excepto el √∫ltimo)
    fields_with_commas = []
    for i, field in enumerate(silver_fields):
        if i < len(silver_fields) - 1:
            fields_with_commas.append(field + ",")
        else:
            fields_with_commas.append(field)
    
    sql = f"""-- Vista Silver para {company_name} - Tabla {table_name}
-- Generada autom√°ticamente el {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
-- Incluye normalizaci√≥n de tipos de datos

CREATE OR REPLACE VIEW `{project_id}.silver.{view_name}` AS (
SELECT
{chr(10).join(fields_with_commas)}
FROM `{project_id}.{dataset_name}.{table_name}`
);
"""
    
    return sql

# Funciones para an√°lisis de tipos de datos
def analyze_data_types_for_table(table_analysis_results):
    """
    Analiza las diferencias de tipos de datos para una tabla
    """
    field_type_analysis = defaultdict(list)
    
    # Recopilar informaci√≥n de tipos por campo
    for result in table_analysis_results:
        company_name = result['company_name']
        project_id = result['project_id']
        fields_df = result['fields_df']
        
        for _, field in fields_df.iterrows():
            field_name = field['column_name']
            data_type = field['data_type']
            
            field_type_analysis[field_name].append({
                'company_name': company_name,
                'project_id': project_id,
                'data_type': data_type,
                'is_nullable': field['is_nullable']
            })
    
    # Analizar diferencias de tipos
    type_conflicts = {}
    field_consensus = {}
    
    for field_name, type_info_list in field_type_analysis.items():
        unique_types = list(set([info['data_type'] for info in type_info_list]))
        
        if len(unique_types) > 1:
            # Hay conflicto de tipos
            type_conflicts[field_name] = {
                'types': unique_types,
                'companies': type_info_list,
                'consensus_type': determine_consensus_type(unique_types, type_info_list)
            }
        else:
            # No hay conflicto
            field_consensus[field_name] = {
                'type': unique_types[0],
                'companies': type_info_list
            }
    
    return field_consensus, type_conflicts

def determine_consensus_type(types, type_info_list):
    """
    Determina el tipo de consenso para un campo con conflictos
    """
    type_priority = {
        'STRING': 1,      # M√°s flexible
        'INT64': 2,       
        'FLOAT64': 3,     
        'BOOL': 4,        
        'DATE': 5,        
        'DATETIME': 6,    
        'TIMESTAMP': 7,   
        'JSON': 8,        
        'BYTES': 9        
    }
    
    # Si hay STRING, usar STRING (m√°s flexible)
    if 'STRING' in types:
        return 'STRING'
    
    # Si hay FLOAT64, usar FLOAT64 (puede contener enteros)
    if 'FLOAT64' in types:
        return 'FLOAT64'
    
    # Si hay INT64, usar INT64
    if 'INT64' in types:
        return 'INT64'
    
    # Para otros tipos, usar el de mayor prioridad
    min_priority = min([type_priority.get(t, 999) for t in types])
    for t in types:
        if type_priority.get(t, 999) == min_priority:
            return t
    
    return types[0]

def generate_cast_for_field(field_name, source_type, target_type):
    """
    Genera la expresi√≥n CAST apropiada para un campo
    """
    if source_type == target_type:
        return field_name
    
    # Mapeo de conversiones seguras
    safe_casts = {
        ('INT64', 'STRING'): f"CAST({field_name} AS STRING)",
        ('INT64', 'FLOAT64'): f"CAST({field_name} AS FLOAT64)",
        ('FLOAT64', 'STRING'): f"CAST({field_name} AS STRING)",
        ('STRING', 'INT64'): f"SAFE_CAST({field_name} AS INT64)",
        ('STRING', 'FLOAT64'): f"SAFE_CAST({field_name} AS FLOAT64)",
        ('STRING', 'BOOL'): f"SAFE_CAST({field_name} AS BOOL)",
        ('BOOL', 'STRING'): f"CAST({field_name} AS STRING)",
        ('DATE', 'STRING'): f"CAST({field_name} AS STRING)",
        ('DATETIME', 'STRING'): f"CAST({field_name} AS STRING)",
        ('TIMESTAMP', 'STRING'): f"CAST({field_name} AS STRING)",
        # JSON a STRING - usar SAFE_CAST con valor por defecto
        ('JSON', 'STRING'): f"COALESCE(SAFE_CAST({field_name} AS STRING), '')"
    }
    
    cast_key = (source_type, target_type)
    if cast_key in safe_casts:
        return safe_casts[cast_key]
    
    # Para conversiones no seguras, usar SAFE_CAST con valor por defecto
    return f"COALESCE(SAFE_CAST({field_name} AS {target_type}), {get_default_value_for_type(target_type)})"

def get_default_value_for_type(data_type):
    """Obtiene el valor por defecto para un tipo de datos"""
    defaults = {
        'STRING': "''",
        'INT64': '0',
        'FLOAT64': '0.0',
        'BOOL': 'FALSE',
        'DATE': 'NULL',
        'DATETIME': 'NULL',
        'TIMESTAMP': 'NULL',
        'JSON': 'NULL',
        'BYTES': 'NULL'
    }
    return defaults.get(data_type, 'NULL')

def generate_all_silver_views():
    """
    Genera vistas Silver para todas las tablas identificadas con seguimiento de estados
    """
    # print("üöÄ Iniciando generaci√≥n de vistas Silver para todas las tablas")
    
    # Inicializar gestor de estados
    status_manager = ConsolidationStatusManager()
    
    # Obtener compa√±√≠as pendientes de consolidaci√≥n
    pending_companies = status_manager.get_companies_for_consolidation()
    
    if pending_companies.empty:
        print("‚ÑπÔ∏è  No hay compa√±√≠as pendientes de consolidaci√≥n")
        return {}, {}
    
    # print(f"üìã Compa√±√≠as a procesar: {len(pending_companies)}")
    
    # Usar configuraci√≥n centralizada
    tables_to_process = TABLES_TO_PROCESS
    
    all_results = {}
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Crear directorio para archivos SQL
    output_dir = f"{OUTPUT_BASE_DIR}/silver_views_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)
    
    # print(f"üöÄ INICIANDO GENERACI√ìN DE VISTAS SILVER")
    # print(f"üìÅ Directorio de salida: {output_dir}")
    # print(f"üìã Tablas a procesar: {len(tables_to_process)}")
    # print("=" * 80)
    
    for table_name in tables_to_process:
        print(f"\nüîÑ Procesando tabla: {table_name}")
        
        # Analizar campos de la tabla
        table_analysis = analyze_table_fields_across_companies(table_name)
        
        if table_analysis is None:
            print(f"  ‚è≠Ô∏è  Saltando tabla '{table_name}' - no se encontraron datos")
            continue
        
        all_results[table_name] = table_analysis
        
        # Generar vistas Silver para cada compa√±√≠a
        company_sql_files = []
        
        for company_result in table_analysis['company_results']:
            company_name = company_result['company_name']
            project_id = company_result['project_id']
            
            # Generar y ejecutar SQL directamente
            sql_content = generate_silver_view_sql(table_analysis, company_result)
            
            # Ejecutar vista directamente en BigQuery
            try:
                # print(f"    üîÑ Creando vista: {project_id}.silver.vw_{table_name}")
                query_job = client.query(sql_content)
                query_job.result()  # Esperar a que termine
                # print(f"    ‚úÖ Vista creada: {company_name}")
                company_sql_files.append(f"SUCCESS: {company_name}")
            except Exception as e:
                print(f"    ‚ùå Error creando vista {company_name}: {str(e)}")
                company_sql_files.append(f"ERROR: {company_name}")
        
        # Crear archivo consolidado para la tabla
        consolidated_filename = f"{output_dir}/consolidated_{table_name}_analysis.sql"
        with open(consolidated_filename, 'w', encoding='utf-8') as f:
            f.write(f"-- An√°lisis consolidado para tabla: {table_name}\n")
            f.write(f"-- Generado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write(f"-- Resumen:\n")
            f.write(f"-- Total compa√±√≠as: {table_analysis['total_companies']}\n")
            f.write(f"-- Campos comunes: {len(table_analysis['common_fields'])}\n")
            f.write(f"-- Campos parciales: {len(table_analysis['partial_fields'])}\n\n")
            
            f.write(f"-- Campos comunes:\n")
            for field in table_analysis['common_fields']:
                f.write(f"--   - {field}\n")
            
            f.write(f"\n-- Campos parciales:\n")
            for field in table_analysis['partial_fields']:
                count = table_analysis['field_frequency'][field]
                f.write(f"--   - {field}: {count}/{table_analysis['total_companies']} compa√±√≠as\n")
            
            f.write(f"\n-- Archivos SQL generados:\n")
            for sql_file in company_sql_files:
                f.write(f"--   - {sql_file}\n")
        
        print(f"    üìÑ Archivo consolidado: {consolidated_filename}")
    
    # Generar resumen final
    summary_filename = f"{output_dir}/GENERATION_SUMMARY.md"
    with open(summary_filename, 'w', encoding='utf-8') as f:
        f.write(f"# Resumen de Generaci√≥n de Vistas Silver\n\n")
        f.write(f"**Fecha de generaci√≥n:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write(f"## Tablas Procesadas\n\n")
        f.write(f"Total de tablas procesadas: {len(all_results)}\n\n")
        
        for table_name, analysis in all_results.items():
            f.write(f"### {table_name}\n")
            f.write(f"- Compa√±√≠as con la tabla: {analysis['total_companies']}\n")
            f.write(f"- Campos comunes: {len(analysis['common_fields'])}\n")
            f.write(f"- Campos parciales: {len(analysis['partial_fields'])}\n\n")
        
        f.write(f"## Archivos Generados\n\n")
        f.write(f"Para cada tabla se generaron:\n")
        f.write(f"- Un archivo SQL por compa√±√≠a: `silver_{{table_name}}_{{project_id}}.sql`\n")
        f.write(f"- Un archivo de an√°lisis consolidado: `consolidated_{{table_name}}_analysis.sql`\n\n")
        
        f.write(f"## Pr√≥ximos Pasos\n\n")
        f.write(f"1. Revisar los archivos SQL generados\n")
        f.write(f"2. Ejecutar las vistas Silver en cada proyecto de compa√±√≠a\n")
        f.write(f"3. Crear las vistas consolidadas en el proyecto central\n")
    
    # Actualizar estados de compa√±√≠as procesadas
    print(f"\nüìä Actualizando estados de consolidaci√≥n...")
    for _, company in pending_companies.iterrows():
        company_id = company['company_id']
        company_name = company['company_name']
        
        # Verificar si la compa√±√≠a fue procesada exitosamente
        company_success = True
        for table_name in tables_to_process:
            if table_name in all_results:
                # Verificar si esta compa√±√≠a tiene datos para esta tabla
                company_has_table = any(
                    result['company_id'] == company_id 
                    for result in all_results[table_name]['company_results']
                )
                if not company_has_table:
                    # No es un error si la compa√±√≠a no tiene cierta tabla
                    continue
        
        # Actualizar estado
        if company_success:
            status_manager.update_company_status(company_id, status_manager.STATUS['COMPLETED'])
            # print(f"  ‚úÖ {company_name}: Estado actualizado a COMPLETED")
        else:
            status_manager.update_company_status(company_id, status_manager.STATUS['ERROR'])
            print(f"  ‚ùå {company_name}: Estado actualizado a ERROR")
    
    # Mostrar resumen de estados
    status_manager.print_consolidation_summary()
    
    print(f"\nüéØ GENERACI√ìN COMPLETADA")
    print(f"üìÅ Directorio: {output_dir}")
    print(f"üìä Tablas procesadas: {len(all_results)}")
    print(f"üìÑ Resumen: {summary_filename}")
    
    return all_results, output_dir

if __name__ == "__main__":
    # Ejecutar generaci√≥n
    results, output_dir = generate_all_silver_views()
    
    # print(f"\n‚úÖ Script completado exitosamente!")
    # print(f"üìÅ Revisa los archivos en: {output_dir}")
