# ğŸš€ Cloud Run Job: Crear Tablas Consolidadas

## ğŸ“‹ DescripciÃ³n

Este Cloud Run Job crea **tablas consolidadas optimizadas** en `pph-central.bronze` a partir de las vistas Silver de cada compaÃ±Ã­a.

## ğŸ¯ CaracterÃ­sticas

- âœ… **Particionamiento por MES** (`DATE_TRUNC(created_on, MONTH)`) - Evita lÃ­mite de 4000 particiones
- âœ… **Clusterizado** segÃºn metadatos de `management.metadata_consolidated_tables`
- âœ… **No interactivo** - Procesa todas las tablas automÃ¡ticamente
- âœ… **Timeout de 1 hora** - Suficiente para procesar todas las tablas
- âœ… **8GB RAM + 4 CPUs** - Recursos suficientes para queries grandes

## ğŸ“Š Flujo del Job

```
1. Cargar metadatos (partition_fields, cluster_fields)
   â†“
2. Obtener tablas con vistas Silver disponibles
   â†“
3. Para cada tabla:
   - Obtener compaÃ±Ã­as que tienen la vista
   - Construir UNION ALL de todas las vistas
   - Crear tabla con PARTITION BY y CLUSTER BY
   â†“
4. Resumen final (Ã©xitos, errores, saltadas)
```

## ğŸ”§ ConfiguraciÃ³n

### Proyecto y Datasets

- **Proyecto Central:** `pph-central`
- **Dataset Bronze:** `bronze` (tablas consolidadas)
- **Proyecto Source:** `platform-partners-pro` (metadatos y companies)
- **Dataset Silver:** `silver` (vistas por compaÃ±Ã­a)

### Service Account

```
data-analytics@platform-partners-pro.iam.gserviceaccount.com
```

**Permisos requeridos:**
- âœ… `bigquery.dataViewer` en proyectos de compaÃ±Ã­as
- âœ… `bigquery.dataEditor` en `pph-central` (para crear tablas)
- âœ… `bigquery.jobUser` en todos los proyectos

### Recursos del Job

- **Memoria:** 8 GB
- **CPU:** 4 cores
- **Timeout:** 3600 segundos (1 hora)
- **Max Retries:** 3

## ğŸš€ Despliegue

### Paso 1: Build y Deploy

```bash
cd bq_management/consolidated_central_project_data
chmod +x build_deploy_consolidated.sh
./build_deploy_consolidated.sh
```

Este script:
1. Crea imagen Docker con `consolidated_tables_job.py`
2. Sube la imagen a `gcr.io/pph-central/create-consolidated-tables-job`
3. Crea/actualiza el Cloud Run Job

### Paso 2: Ejecutar el Job

```bash
gcloud run jobs execute create-consolidated-tables-job \
  --region=us-east1 \
  --project=pph-central
```

### Paso 3: Monitorear Logs

```bash
# Ver logs en tiempo real
gcloud run jobs logs create-consolidated-tables-job \
  --region=us-east1 \
  --project=pph-central

# Ver logs en Cloud Console
# https://console.cloud.google.com/run/jobs/details/us-east1/create-consolidated-tables-job
```

## ğŸ“Š Estructura de Tablas Consolidadas

### Naming Convention

```
pph-central.bronze.consolidated_{table_name}
```

Ejemplos:
- `consolidated_appointment`
- `consolidated_customer`
- `consolidated_invoice`

### Schema

Todas las tablas consolidadas tienen estos campos adicionales:

```sql
company_project_id STRING   -- Proyecto de la compaÃ±Ã­a (ej: "shape-mhs-1")
company_id INT64            -- ID de la compaÃ±Ã­a (ej: 1)
... [campos originales de la tabla] ...
```

### Particionamiento

```sql
PARTITION BY DATE_TRUNC(created_on, MONTH)
```

**Â¿Por quÃ© MES y no DÃA?**
- LÃ­mite de BigQuery: 4000 particiones mÃ¡ximo
- Por dÃ­a: 4000 dÃ­as = ~11 aÃ±os
- Por mes: 4000 meses = ~333 aÃ±os âœ…

### Clusterizado

SegÃºn metadatos en `management.metadata_consolidated_tables`:

```sql
CLUSTER BY company_id, [otros campos segÃºn tabla]
```

**Beneficios:**
- âœ… Queries mÃ¡s rÃ¡pidos al filtrar por compaÃ±Ã­a
- âœ… Menor costo de queries
- âœ… Mejor organizaciÃ³n de datos

## ğŸ” ValidaciÃ³n

### Verificar que las tablas se crearon

```sql
SELECT 
  table_name,
  row_count,
  size_bytes,
  ROUND(size_bytes / POW(1024, 3), 2) as size_gb,
  creation_time
FROM `pph-central.bronze.__TABLES__`
WHERE table_id LIKE 'consolidated_%'
ORDER BY creation_time DESC;
```

### Ver distribuciÃ³n de datos por compaÃ±Ã­a

```sql
SELECT 
  company_project_id,
  company_id,
  COUNT(*) as total_records
FROM `pph-central.bronze.consolidated_appointment`
GROUP BY company_project_id, company_id
ORDER BY total_records DESC;
```

### Verificar particionamiento

```sql
SELECT 
  partition_id,
  total_rows,
  ROUND(total_logical_bytes / POW(1024, 3), 2) as size_gb
FROM `pph-central.bronze.INFORMATION_SCHEMA.PARTITIONS`
WHERE table_name = 'consolidated_appointment'
ORDER BY partition_id DESC
LIMIT 10;
```

## âš ï¸ Troubleshooting

### Error: Too many partitions

**Problema:** MÃ¡s de 4000 dÃ­as de datos

**SoluciÃ³n:** El Job ya usa `DATE_TRUNC(created_on, MONTH)`. Si aÃºn falla:
1. Cambiar a `DATE_TRUNC(created_on, YEAR)`
2. O filtrar datos histÃ³ricos

### Error: Timeout

**Problema:** El Job no termina en 1 hora

**SoluciÃ³n:**
1. Aumentar `--task-timeout` a 7200 (2 horas)
2. O procesar tablas en lotes

### Error: Out of Memory

**Problema:** Query muy grande para 8GB

**SoluciÃ³n:**
1. Aumentar `--memory` a 16Gi
2. Reducir nÃºmero de compaÃ±Ã­as por lote

### Error: Permission Denied

**Problema:** Service Account sin permisos

**SoluciÃ³n:**
```bash
# Para cada proyecto de compaÃ±Ã­a
gcloud projects add-iam-policy-binding COMPANY_PROJECT_ID \
  --member="serviceAccount:data-analytics@platform-partners-pro.iam.gserviceaccount.com" \
  --role="roles/bigquery.dataViewer"
```

## ğŸ“ Logs de Ejemplo

### Ã‰xito

```
ğŸš€ CLOUD RUN JOB - CREAR TABLAS CONSOLIDADAS
================================================================================
âœ… Metadatos cargados: 24 tablas
âœ… Tablas disponibles: 24

[1/24] appointment
  ğŸ”„ Creando tabla: consolidated_appointment
     ğŸ“Š CompaÃ±Ã­as: 30
     âš™ï¸  Particionado: created_on (por MES)
     ğŸ”— Clusterizado: ['company_id']
  âœ… Tabla creada exitosamente

...

================================================================================
ğŸ¯ RESUMEN FINAL
================================================================================
âœ… Tablas creadas exitosamente: 24
âŒ Tablas con errores: 0
â­ï¸  Tablas saltadas: 0
ğŸ“Š Total procesadas: 24
```

### Con Errores

```
[5/24] customer
  ğŸ”„ Creando tabla: consolidated_customer
     ğŸ“Š CompaÃ±Ã­as: 28
  âŒ Error: Column 'email' type mismatch...

================================================================================
ğŸ¯ RESUMEN FINAL
================================================================================
âœ… Tablas creadas exitosamente: 23
âŒ Tablas con errores: 1
â­ï¸  Tablas saltadas: 0
ğŸ“Š Total procesadas: 24
âš ï¸  Algunas tablas tuvieron errores. Revisa los logs arriba.
```

## ğŸ”„ Re-ejecuciÃ³n

El Job usa `CREATE OR REPLACE TABLE`, por lo que:
- âœ… Es seguro re-ejecutar
- âœ… Sobrescribe tablas existentes
- âœ… No duplica datos
- âš ï¸ Destruye datos antiguos (si cambiaste la lÃ³gica)

## ğŸ“… Mantenimiento

### Actualizar el Job

```bash
# 1. Modificar consolidated_tables_job.py
# 2. Re-deployar
./build_deploy_consolidated.sh
```

### Eliminar el Job

```bash
gcloud run jobs delete create-consolidated-tables-job \
  --region=us-east1 \
  --project=pph-central
```

### Ver historial de ejecuciones

```bash
gcloud run jobs executions list \
  --job=create-consolidated-tables-job \
  --region=us-east1 \
  --project=pph-central
```

## ğŸ¯ PrÃ³ximos Pasos

DespuÃ©s de crear las tablas consolidadas en Bronze:

1. **Paso 4:** Crear vistas consolidadas en `pph-central.silver`
2. **Paso 5:** Configurar permisos para usuarios finales
3. **Paso 6:** Crear dashboards en Looker/Tableau

---

**Fecha de creaciÃ³n:** 2025-10-08  
**VersiÃ³n:** 1.0  
**Autor:** Data Engineering Team

