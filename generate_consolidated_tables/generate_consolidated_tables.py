#!/usr/bin/env python3
"""
Cloud Run Job: Crear Tablas Consolidadas en pph-central.bronze
Versi√≥n NO INTERACTIVA - Procesa todas las tablas disponibles
"""

from google.cloud import bigquery
from google.cloud import bigquery_datatransfer_v1
import pandas as pd
from datetime import datetime
import sys
import json

# Configuraci√≥n
PROJECT_CENTRAL = 'pph-central'            # Proyecto central √∫nico
DATASET_SETTINGS = 'settings'              # Configuraci√≥n de compa√±√≠as
DATASET_BRONZE = 'bronze'                  # Tablas consolidadas destino
DATASET_SILVER = 'silver'                  # Vistas Silver (en proyectos shape-*)
DATASET_MANAGEMENT = 'management'          # Metadatos y gobierno

# Clientes BigQuery
client = bigquery.Client(project=PROJECT_CENTRAL)
transfer_client = bigquery_datatransfer_v1.DataTransferServiceClient()

def get_metadata_dict():
    """Obtiene metadatos de particionamiento y clusterizado"""
    query = f"""
        SELECT 
            table_name,
            partition_fields,
            cluster_fields
        FROM `{PROJECT_CENTRAL}.{DATASET_MANAGEMENT}.metadata_consolidated_tables`
        ORDER BY table_name
    """
    
    print(f"üìã Cargando metadatos desde: {PROJECT_CENTRAL}.{DATASET_MANAGEMENT}.metadata_consolidated_tables")
    
    try:
        query_job = client.query(query)
        results = query_job.result()
        df = pd.DataFrame([dict(row) for row in results])
        
        print(f"üîç DEBUG: Filas obtenidas de metadatos: {len(df)}")
        
        if len(df) > 0:
            print(f"üìã Primeras 5 tablas en metadatos:")
            for i, row in df.head(5).iterrows():
                print(f"   - {row['table_name']}: partition={row['partition_fields']}, cluster={row['cluster_fields']}")
        
        metadata_dict = {}
        for _, row in df.iterrows():
            metadata_dict[row['table_name']] = {
                'partition_fields': row['partition_fields'],
                'cluster_fields': row['cluster_fields']
            }
        
        print(f"‚úÖ Metadatos cargados: {len(metadata_dict)} tablas")
        print(f"üìã Tablas en diccionario: {list(metadata_dict.keys())[:10]}")
        return metadata_dict
    except Exception as e:
        print(f"‚ö†Ô∏è  Error cargando metadatos: {str(e)}")
        print("   Usando configuraci√≥n por defecto para todas las tablas")
        return {}

def get_available_tables():
    """
    Obtiene lista de tablas desde METADATOS (no desde vistas Silver)
    Los metadatos son la GU√çA del proceso
    """
    query = f"""
        SELECT table_name
        FROM `{PROJECT_CENTRAL}.{DATASET_MANAGEMENT}.metadata_consolidated_tables`
        ORDER BY table_name
    """
    
    try:
        query_job = client.query(query)
        results = query_job.result()
        tables = [row.table_name for row in results]
        
        print(f"‚úÖ Tablas desde metadatos: {len(tables)}")
        return tables
        
    except Exception as e:
        print(f"‚ùå Error obteniendo tablas desde metadatos: {str(e)}")
        return []

def get_companies_for_table(table_name):
    """Obtiene compa√±√≠as que tienen una vista Silver espec√≠fica"""
    query = f"""
        SELECT 
            company_id,
            company_name,
            company_project_id
        FROM `{PROJECT_CENTRAL}.{DATASET_SETTINGS}.companies`
        WHERE company_fivetran_status = TRUE
          AND company_bigquery_status = TRUE
          AND company_project_id IS NOT NULL
        ORDER BY company_id
    """
    
    try:
        query_job = client.query(query)
        results = query_job.result()
        all_companies = pd.DataFrame([dict(row) for row in results])
        
        # Filtrar compa√±√≠as que tienen la vista
        companies_with_view = []
        
        for _, company in all_companies.iterrows():
            try:
                # Verificar si la vista existe en esta compa√±√≠a
                check_query = f"""
                    SELECT 1
                    FROM `{company['company_project_id']}.silver.INFORMATION_SCHEMA.TABLES`
                    WHERE table_name = 'vw_{table_name}'
                    LIMIT 1
                """
                
                check_job = client.query(check_query)
                check_results = list(check_job.result())
                
                if check_results:
                    companies_with_view.append(company)
                    
            except Exception:
                # Saltar compa√±√≠as sin la vista
                continue
        
        df = pd.DataFrame(companies_with_view)
        return df
        
    except Exception as e:
        print(f"  ‚ö†Ô∏è  Error obteniendo compa√±√≠as: {str(e)}")
        return pd.DataFrame()

def verify_field_exists(table_name, field_name, company_project_id):
    """Verifica si un campo existe en una vista Silver"""
    try:
        query = f"""
            SELECT 1
            FROM `{company_project_id}.silver.INFORMATION_SCHEMA.COLUMNS`
            WHERE table_name = 'vw_{table_name}'
              AND column_name = '{field_name}'
            LIMIT 1
        """
        
        result = client.query(query).result()
        return len(list(result)) > 0
    except Exception:
        return False

def detect_partition_field(table_name, sample_company_project_id):
    """Detecta un campo de fecha apropiado para particionar"""
    # Lista de campos comunes de fecha (en orden de preferencia)
    date_fields = ['created_on', 'created_at', 'date', 'timestamp', 'modified_on', 'updated_on']
    
    try:
        # Obtener schema de la vista
        schema_query = f"""
            SELECT column_name, data_type
            FROM `{sample_company_project_id}.silver.INFORMATION_SCHEMA.COLUMNS`
            WHERE table_name = 'vw_{table_name}'
              AND data_type IN ('TIMESTAMP', 'DATETIME', 'DATE')
            ORDER BY ordinal_position
        """
        
        query_job = client.query(schema_query)
        results = query_job.result()
        date_columns = [row.column_name for row in results]
        
        # Buscar el primer campo de fecha com√∫n
        for field in date_fields:
            if field in date_columns:
                return field
        
        # Si no encontr√≥ ninguno com√∫n, usar el primero disponible
        if date_columns:
            return date_columns[0]
        
        # Sin campos de fecha
        return None
        
    except Exception as e:
        print(f"    ‚ö†Ô∏è  No se pudo detectar campo de fecha: {str(e)}")
        return None

def create_consolidated_table(table_name, companies_df, metadata_dict):
    """Crea una tabla consolidada para una tabla espec√≠fica"""
    
    if companies_df.empty:
        print(f"  ‚ùå No hay compa√±√≠as disponibles")
        return False, None, []
    
    # Obtener metadatos
    if table_name in metadata_dict:
        metadata = metadata_dict[table_name]
        print(f"  üîç DEBUG: Metadatos encontrados para '{table_name}'")
        print(f"     partition_fields: {metadata['partition_fields']} (tipo: {type(metadata['partition_fields'])})")
        print(f"     cluster_fields: {metadata['cluster_fields']} (tipo: {type(metadata['cluster_fields'])})")
        
        # Manejar arrays de BigQuery
        partition_fields_list = list(metadata['partition_fields']) if metadata['partition_fields'] else []
        cluster_fields_list = list(metadata['cluster_fields']) if metadata['cluster_fields'] else []
        
        # PARTICIONAMIENTO: Intentar campos en orden (tipo COALESCE)
        # Solo se usa 1 campo, pero se prueban varios por si el primero no existe
        partition_field = None
        if partition_fields_list:
            for field in partition_fields_list:
                # Verificar si el campo existe en la vista
                if verify_field_exists(table_name, field, companies_df.iloc[0]['company_project_id']):
                    partition_field = field
                    print(f"  ‚úÖ Campo de particionamiento seleccionado: {field}")
                    break
                else:
                    print(f"  ‚ö†Ô∏è  Campo '{field}' no existe en la vista, probando siguiente...")
        
        # CLUSTERIZADO: Usar todos los campos (hasta 4)
        cluster_fields = cluster_fields_list[:4] if cluster_fields_list else ['company_id']
    else:
        print(f"  ‚ö†Ô∏è  Tabla '{table_name}' NO est√° en metadatos")
        partition_field = None
        cluster_fields = ['company_id']
    
    # Si no hay partition_field, intentar detectarlo autom√°ticamente
    if not partition_field:
        print(f"  üîç Detectando campo de particionamiento autom√°ticamente...")
        partition_field = detect_partition_field(table_name, companies_df.iloc[0]['company_project_id'])
        
        if not partition_field:
            print(f"  ‚ùå ERROR: No se encontr√≥ campo de fecha para particionar")
            print(f"     Soluci√≥n: Agregar tabla '{table_name}' a metadata_consolidated_tables")
            print(f"     con un partition_field apropiado (created_on, created_at, etc.)")
            return False, None, []
    
    # Construir UNION ALL con metadata de compa√±√≠a
    union_parts = []
    for _, company in companies_df.iterrows():
        union_part = f"""
        SELECT 
          '{company['company_project_id']}' AS company_project_id,
          {company['company_id']} AS company_id,
          *
        FROM `{company['company_project_id']}.{DATASET_SILVER}.vw_{table_name}`"""
        union_parts.append(union_part)
    
    # Configurar clusterizado
    cluster_sql = f"CLUSTER BY {', '.join(cluster_fields)}" if cluster_fields else ""
    
    # SQL completo - SIEMPRE con particionamiento por MES
    create_sql = f"""
    CREATE OR REPLACE TABLE `{PROJECT_CENTRAL}.{DATASET_BRONZE}.consolidated_{table_name}`
    PARTITION BY DATE_TRUNC({partition_field}, MONTH)
    {cluster_sql}
    AS
    {' UNION ALL '.join(union_parts)}
    """
    
    print(f"  üîÑ Creando tabla: consolidated_{table_name}")
    print(f"     üìä Compa√±√≠as: {len(companies_df)}")
    print(f"     ‚öôÔ∏è  Particionado: {partition_field} (por MES)")
    print(f"     üîó Clusterizado: {cluster_fields}")
    
    try:
        query_job = client.query(create_sql)
        query_job.result()
        print(f"  ‚úÖ Tabla creada exitosamente")
        return True, partition_field, cluster_fields
    except Exception as e:
        error_msg = str(e)
        
        # Detectar tipo de error espec√≠fico
        if "Cannot replace a table with a different" in error_msg:
            print(f"  ‚ö†Ô∏è  TABLA NO ACTUALIZADA: La tabla existente tiene configuraci√≥n incompatible")
            print(f"     Raz√≥n: Cambio de esquema de particionamiento (d√≠a‚Üímes) o estructura")
            print(f"     Acci√≥n: La tabla antigua se mantiene intacta (seguro)")
            print(f"     Para actualizar: Eliminar manualmente cuando no haya dependencias")
        elif "Unrecognized name" in error_msg:
            campo_error = error_msg.split("Unrecognized name: ")[1].split(" ")[0] if "Unrecognized name: " in error_msg else "desconocido"
            print(f"  ‚ùå ERROR: Campo '{campo_error}' no existe en las vistas Silver")
            print(f"     Soluci√≥n: Verificar partition_fields en metadatos para '{table_name}'")
        elif "Too many partitions" in error_msg:
            print(f"  ‚ùå ERROR: Demasiadas particiones (l√≠mite: 4000)")
            print(f"     Soluci√≥n: Cambiar particionamiento a YEAR o filtrar datos hist√≥ricos")
        else:
            # Error gen√©rico - mostrar primeras 300 caracteres
            if len(error_msg) > 300:
                error_msg = error_msg[:300] + "..."
            print(f"  ‚ùå ERROR: {error_msg}")
        
        return False, None, []

def create_or_update_scheduled_query(table_name, companies_df, partition_field, cluster_fields):
    """
    Crea o actualiza un Scheduled Query para refresh diario de la tabla consolidada
    
    NOTA: Esta funci√≥n crea la configuraci√≥n del scheduled query.
    Para que funcione completamente, se requiere:
    1. Habilitar BigQuery Data Transfer API
    2. Permisos del Service Account en todos los proyectos de compa√±√≠as
    """
    display_name = f"sq_consolidated_{table_name}"
    
    # Construir query de refresh con MERGE (at√≥mico y seguro)
    # Clave compuesta: company_project_id + id (√∫nico en tabla consolidada)
    # Sin filtro temporal - procesa TODAS las vistas para capturar cualquier actualizaci√≥n
    
    union_parts = []
    for _, company in companies_df.iterrows():
        union_part = f"""
        SELECT 
          '{company['company_project_id']}' AS company_project_id,
          {company['company_id']} AS company_id,
          *
        FROM `{company['company_project_id']}.{DATASET_SILVER}.vw_{table_name}`"""
        union_parts.append(union_part)
    
    refresh_sql = f"""
/*
 * Refresh completo para {table_name}
 * Recrea la tabla completa desde las vistas Silver
 * Mantiene particionamiento y clusterizado originales
 * Generado autom√°ticamente
 */
CREATE OR REPLACE TABLE `{PROJECT_CENTRAL}.{DATASET_BRONZE}.consolidated_{table_name}`
PARTITION BY DATE_TRUNC({partition_field}, MONTH)
CLUSTER BY ({', '.join(cluster_fields)})
AS
{' UNION ALL '.join(union_parts)};
"""
    
    try:
        parent = f"projects/{PROJECT_CENTRAL}/locations/us"
        
        # Configuraci√≥n del scheduled query
        transfer_config = bigquery_datatransfer_v1.TransferConfig(
            display_name=display_name,
            data_source_id="scheduled_query",
            schedule="every 6 hours",  # Corre cada 6 horas (aligned con Fivetran)
            disabled=True,  # Crear DESHABILITADO para sincronizaci√≥n perfecta
            params={
                "query": refresh_sql
                # No se especifica write_disposition porque CREATE OR REPLACE maneja el reemplazo
            }
        )
        
        # Buscar si ya existe
        list_request = bigquery_datatransfer_v1.ListTransferConfigsRequest(
            parent=parent,
            data_source_ids=["scheduled_query"]
        )
        
        existing_config = None
        for config in transfer_client.list_transfer_configs(request=list_request):
            if config.display_name == display_name:
                existing_config = config
                break
        
        if existing_config:
            # Actualizar existente
            transfer_config.name = existing_config.name
            update_mask = {"paths": ["schedule", "params"]}
            transfer_client.update_transfer_config(
                transfer_config=transfer_config,
                update_mask=update_mask
            )
            print(f"  üîÑ Scheduled Query actualizado")
        else:
            # Crear nuevo
            transfer_client.create_transfer_config(
                parent=parent,
                transfer_config=transfer_config
            )
            print(f"  ‚úÖ Scheduled Query creado")
        
        return True
        
    except Exception as e:
        error_str = str(e)
        if "API has not been used" in error_str or "not been enabled" in error_str:
            print(f"  ‚ö†Ô∏è  BigQuery Data Transfer API no habilitada")
            print(f"     Habilitar: gcloud services enable bigquerydatatransfer.googleapis.com")
        else:
            print(f"  ‚ö†Ô∏è  Error creando Scheduled Query: {error_str[:250]}")
        
        print(f"     Tabla creada OK - Scheduled Query se puede crear manualmente despu√©s")
        return False

def create_all_consolidated_tables(create_schedules=True, start_from_letter='a', specific_table=None):
    """
    Funci√≥n principal para crear tablas consolidadas
    
    Args:
        create_schedules (bool): Si True, crea scheduled queries para refresh autom√°tico
        start_from_letter (str): Letra inicial para filtrar tablas (√∫til para reiniciar)
        specific_table (str): Si se proporciona, genera solo esta tabla
        
    Returns:
        dict: Estad√≠sticas de ejecuci√≥n
    """
    print("=" * 80)
    print("üöÄ CREAR TABLAS CONSOLIDADAS")
    print(f"   Proyecto Central: {PROJECT_CENTRAL}")
    print(f"   Dataset Bronze: {DATASET_BRONZE}")
    print(f"   Scheduled Queries: {'S√ç' if create_schedules else 'NO'}")
    print(f"   Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    # 1. Cargar metadatos
    print("\nüìã PASO 1: Cargar metadatos de particionamiento/clusterizado")
    metadata_dict = get_metadata_dict()
    
    # 2. Obtener tablas disponibles
    print("\nüìä PASO 2: Obtener tablas con vistas Silver disponibles")
    all_tables = get_available_tables()
    
    if not all_tables:
        print("‚ùå No se encontraron tablas disponibles")
        sys.exit(1)
    
    # Filtrar tablas seg√∫n los par√°metros
    if specific_table:
        # Procesar solo una tabla espec√≠fica
        if specific_table in all_tables:
            available_tables = [specific_table]
            print(f"üéØ TABLA ESPEC√çFICA: Procesando solo '{specific_table}'")
        else:
            print(f"‚ùå ERROR: La tabla '{specific_table}' no existe")
            sys.exit(1)
    else:
        # Aplicar filtro de letra inicial
        available_tables = [t for t in all_tables if t >= start_from_letter]
        
        if start_from_letter != 'a':
            print(f"üîç FILTRO ACTIVO: Procesando tablas desde '{start_from_letter}'")
        
        print(f"üìã Tablas a procesar: {len(available_tables)} de {len(all_tables)} totales")
    
    # 3. Procesar cada tabla
    print("\nüîÑ PASO 3: Crear tablas consolidadas")
    print("=" * 80)
    
    success_count = 0
    error_count = 0
    skipped_count = 0
    incompatible_count = 0
    error_tables = []
    incompatible_tables = []
    
    for i, table_name in enumerate(available_tables, 1):
        print(f"\n[{i}/{len(available_tables)}] {table_name}")
        
        # Obtener compa√±√≠as para esta tabla
        companies_df = get_companies_for_table(table_name)
        
        if companies_df.empty:
            print(f"  ‚ö†Ô∏è  Sin compa√±√≠as - SALTAR")
            skipped_count += 1
            continue
        
        # Crear tabla consolidada (retorna √©xito, partition_field, cluster_fields)
        table_created, partition_field, cluster_fields = create_consolidated_table(table_name, companies_df, metadata_dict)
        
        if table_created:
            success_count += 1
            
            # Crear scheduled query solo si hay partition_field y est√° habilitado
            if create_schedules and partition_field:
                print(f"  üìÖ Configurando refresh autom√°tico...")
                create_or_update_scheduled_query(table_name, companies_df, partition_field, cluster_fields)
            elif not partition_field:
                print(f"  ‚ö†Ô∏è  Sin partition_field - No se crea scheduled query")
            elif not create_schedules:
                print(f"  ‚è≠Ô∏è  Scheduled queries deshabilitados")
        else:
            error_count += 1
            error_tables.append(table_name)
    
    # 4. Resumen final
    print("\n" + "=" * 80)
    print("üéØ RESUMEN FINAL")
    print("=" * 80)
    print(f"‚úÖ Tablas creadas/actualizadas: {success_count}")
    print(f"‚ùå Tablas con errores: {error_count}")
    print(f"‚è≠Ô∏è  Tablas saltadas (sin compa√±√≠as): {skipped_count}")
    print(f"üìä Total procesadas: {success_count + error_count + skipped_count}")
    print(f"‚è±Ô∏è  Fecha fin: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    if error_tables:
        print(f"\n‚ö†Ô∏è  TABLAS CON ERRORES ({len(error_tables)}):")
        for table in error_tables:
            print(f"   - {table}")
        print(f"\nüí° NOTA: Revisa los logs arriba para detalles de cada error")
        print(f"   Las tablas con 'configuraci√≥n incompatible' mantienen su versi√≥n anterior")
    
    # Instrucciones para Scheduled Queries
    if create_schedules and success_count > 0:
        print(f"\n" + "="*80)
        print(f"üîî IMPORTANTE: SCHEDULED QUERIES CREADOS DESHABILITADOS")
        print(f"="*80)
        print(f"üìã Total de Scheduled Queries creados: {success_count}")
        print(f"‚è∏Ô∏è  Estado: PAUSADOS (para sincronizaci√≥n perfecta)")
        print(f"\n‚úÖ SIGUIENTE PASO - Ejecuta este comando cuando quieras activarlos:")
        print(f"   python generate_consolidated_tables/enable_all_schedules.py")
        print(f"\nüí° Esto habilitar√° TODOS los schedules a la vez de forma sincronizada")
        print(f"   Todos empezar√°n cada 6 horas desde el momento de activaci√≥n")
        print(f"="*80)
    
    # Retornar estad√≠sticas
    return {
        'success_count': success_count,
        'error_count': error_count,
        'skipped_count': skipped_count,
        'error_tables': error_tables,
        'total_tables': len(available_tables)
    }

if __name__ == "__main__":
    import sys
    import argparse
    
    # Configurar argumentos de l√≠nea de comandos
    parser = argparse.ArgumentParser(description='Crea tablas consolidadas en pph-central.bronze')
    parser.add_argument('--no-schedules', action='store_true', help='No crear Scheduled Queries para refresh autom√°tico')
    parser.add_argument('--start-letter', '-s', default='a', help='Letra inicial para filtrar tablas (default: a)')
    parser.add_argument('--table', '-t', help='Procesar solo una tabla espec√≠fica')
    parser.add_argument('--yes', '-y', action='store_true', help='Responder "s√≠" a todas las confirmaciones')
    
    args = parser.parse_args()
    
    try:
        stats = create_all_consolidated_tables(
            create_schedules=not args.no_schedules,
            start_from_letter=args.start_letter,
            specific_table=args.table
        )
        
        if stats['error_count'] > 0:
            print(f"\n‚ö†Ô∏è  Completado con {stats['error_count']} error(es)")
            sys.exit(1)
        else:
            print("\n‚úÖ ¬°Todas las tablas creadas exitosamente!")
            sys.exit(0)
            
    except Exception as e:
        print(f"\n‚ùå ERROR CR√çTICO: {str(e)}")
        sys.exit(1)

